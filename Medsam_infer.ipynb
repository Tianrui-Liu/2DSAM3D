{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from transformers import SamProcessor,SamModel \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import binary_dilation, label, sum as ndi_sum\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, binary_dilation, sum as ndi_sum\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_encoder.pos_embed\n",
      "vision_encoder.patch_embed.projection.weight\n",
      "vision_encoder.patch_embed.projection.bias\n",
      "vision_encoder.layers.0.layer_norm1.weight\n",
      "vision_encoder.layers.0.layer_norm1.bias\n",
      "vision_encoder.layers.0.attn.rel_pos_h\n",
      "vision_encoder.layers.0.attn.rel_pos_w\n",
      "vision_encoder.layers.0.attn.qkv.weight\n",
      "vision_encoder.layers.0.attn.qkv.bias\n",
      "vision_encoder.layers.0.attn.proj.weight\n",
      "vision_encoder.layers.0.attn.proj.bias\n",
      "vision_encoder.layers.0.layer_norm2.weight\n",
      "vision_encoder.layers.0.layer_norm2.bias\n",
      "vision_encoder.layers.0.mlp.lin1.weight\n",
      "vision_encoder.layers.0.mlp.lin1.bias\n",
      "vision_encoder.layers.0.mlp.lin2.weight\n",
      "vision_encoder.layers.0.mlp.lin2.bias\n",
      "vision_encoder.layers.1.layer_norm1.weight\n",
      "vision_encoder.layers.1.layer_norm1.bias\n",
      "vision_encoder.layers.1.attn.rel_pos_h\n",
      "vision_encoder.layers.1.attn.rel_pos_w\n",
      "vision_encoder.layers.1.attn.qkv.weight\n",
      "vision_encoder.layers.1.attn.qkv.bias\n",
      "vision_encoder.layers.1.attn.proj.weight\n",
      "vision_encoder.layers.1.attn.proj.bias\n",
      "vision_encoder.layers.1.layer_norm2.weight\n",
      "vision_encoder.layers.1.layer_norm2.bias\n",
      "vision_encoder.layers.1.mlp.lin1.weight\n",
      "vision_encoder.layers.1.mlp.lin1.bias\n",
      "vision_encoder.layers.1.mlp.lin2.weight\n",
      "vision_encoder.layers.1.mlp.lin2.bias\n",
      "vision_encoder.layers.2.layer_norm1.weight\n",
      "vision_encoder.layers.2.layer_norm1.bias\n",
      "vision_encoder.layers.2.attn.rel_pos_h\n",
      "vision_encoder.layers.2.attn.rel_pos_w\n",
      "vision_encoder.layers.2.attn.qkv.weight\n",
      "vision_encoder.layers.2.attn.qkv.bias\n",
      "vision_encoder.layers.2.attn.proj.weight\n",
      "vision_encoder.layers.2.attn.proj.bias\n",
      "vision_encoder.layers.2.layer_norm2.weight\n",
      "vision_encoder.layers.2.layer_norm2.bias\n",
      "vision_encoder.layers.2.mlp.lin1.weight\n",
      "vision_encoder.layers.2.mlp.lin1.bias\n",
      "vision_encoder.layers.2.mlp.lin2.weight\n",
      "vision_encoder.layers.2.mlp.lin2.bias\n",
      "vision_encoder.layers.3.layer_norm1.weight\n",
      "vision_encoder.layers.3.layer_norm1.bias\n",
      "vision_encoder.layers.3.attn.rel_pos_h\n",
      "vision_encoder.layers.3.attn.rel_pos_w\n",
      "vision_encoder.layers.3.attn.qkv.weight\n",
      "vision_encoder.layers.3.attn.qkv.bias\n",
      "vision_encoder.layers.3.attn.proj.weight\n",
      "vision_encoder.layers.3.attn.proj.bias\n",
      "vision_encoder.layers.3.layer_norm2.weight\n",
      "vision_encoder.layers.3.layer_norm2.bias\n",
      "vision_encoder.layers.3.mlp.lin1.weight\n",
      "vision_encoder.layers.3.mlp.lin1.bias\n",
      "vision_encoder.layers.3.mlp.lin2.weight\n",
      "vision_encoder.layers.3.mlp.lin2.bias\n",
      "vision_encoder.layers.4.layer_norm1.weight\n",
      "vision_encoder.layers.4.layer_norm1.bias\n",
      "vision_encoder.layers.4.attn.rel_pos_h\n",
      "vision_encoder.layers.4.attn.rel_pos_w\n",
      "vision_encoder.layers.4.attn.qkv.weight\n",
      "vision_encoder.layers.4.attn.qkv.bias\n",
      "vision_encoder.layers.4.attn.proj.weight\n",
      "vision_encoder.layers.4.attn.proj.bias\n",
      "vision_encoder.layers.4.layer_norm2.weight\n",
      "vision_encoder.layers.4.layer_norm2.bias\n",
      "vision_encoder.layers.4.mlp.lin1.weight\n",
      "vision_encoder.layers.4.mlp.lin1.bias\n",
      "vision_encoder.layers.4.mlp.lin2.weight\n",
      "vision_encoder.layers.4.mlp.lin2.bias\n",
      "vision_encoder.layers.5.layer_norm1.weight\n",
      "vision_encoder.layers.5.layer_norm1.bias\n",
      "vision_encoder.layers.5.attn.rel_pos_h\n",
      "vision_encoder.layers.5.attn.rel_pos_w\n",
      "vision_encoder.layers.5.attn.qkv.weight\n",
      "vision_encoder.layers.5.attn.qkv.bias\n",
      "vision_encoder.layers.5.attn.proj.weight\n",
      "vision_encoder.layers.5.attn.proj.bias\n",
      "vision_encoder.layers.5.layer_norm2.weight\n",
      "vision_encoder.layers.5.layer_norm2.bias\n",
      "vision_encoder.layers.5.mlp.lin1.weight\n",
      "vision_encoder.layers.5.mlp.lin1.bias\n",
      "vision_encoder.layers.5.mlp.lin2.weight\n",
      "vision_encoder.layers.5.mlp.lin2.bias\n",
      "vision_encoder.layers.6.layer_norm1.weight\n",
      "vision_encoder.layers.6.layer_norm1.bias\n",
      "vision_encoder.layers.6.attn.rel_pos_h\n",
      "vision_encoder.layers.6.attn.rel_pos_w\n",
      "vision_encoder.layers.6.attn.qkv.weight\n",
      "vision_encoder.layers.6.attn.qkv.bias\n",
      "vision_encoder.layers.6.attn.proj.weight\n",
      "vision_encoder.layers.6.attn.proj.bias\n",
      "vision_encoder.layers.6.layer_norm2.weight\n",
      "vision_encoder.layers.6.layer_norm2.bias\n",
      "vision_encoder.layers.6.mlp.lin1.weight\n",
      "vision_encoder.layers.6.mlp.lin1.bias\n",
      "vision_encoder.layers.6.mlp.lin2.weight\n",
      "vision_encoder.layers.6.mlp.lin2.bias\n",
      "vision_encoder.layers.7.layer_norm1.weight\n",
      "vision_encoder.layers.7.layer_norm1.bias\n",
      "vision_encoder.layers.7.attn.rel_pos_h\n",
      "vision_encoder.layers.7.attn.rel_pos_w\n",
      "vision_encoder.layers.7.attn.qkv.weight\n",
      "vision_encoder.layers.7.attn.qkv.bias\n",
      "vision_encoder.layers.7.attn.proj.weight\n",
      "vision_encoder.layers.7.attn.proj.bias\n",
      "vision_encoder.layers.7.layer_norm2.weight\n",
      "vision_encoder.layers.7.layer_norm2.bias\n",
      "vision_encoder.layers.7.mlp.lin1.weight\n",
      "vision_encoder.layers.7.mlp.lin1.bias\n",
      "vision_encoder.layers.7.mlp.lin2.weight\n",
      "vision_encoder.layers.7.mlp.lin2.bias\n",
      "vision_encoder.layers.8.layer_norm1.weight\n",
      "vision_encoder.layers.8.layer_norm1.bias\n",
      "vision_encoder.layers.8.attn.rel_pos_h\n",
      "vision_encoder.layers.8.attn.rel_pos_w\n",
      "vision_encoder.layers.8.attn.qkv.weight\n",
      "vision_encoder.layers.8.attn.qkv.bias\n",
      "vision_encoder.layers.8.attn.proj.weight\n",
      "vision_encoder.layers.8.attn.proj.bias\n",
      "vision_encoder.layers.8.layer_norm2.weight\n",
      "vision_encoder.layers.8.layer_norm2.bias\n",
      "vision_encoder.layers.8.mlp.lin1.weight\n",
      "vision_encoder.layers.8.mlp.lin1.bias\n",
      "vision_encoder.layers.8.mlp.lin2.weight\n",
      "vision_encoder.layers.8.mlp.lin2.bias\n",
      "vision_encoder.layers.9.layer_norm1.weight\n",
      "vision_encoder.layers.9.layer_norm1.bias\n",
      "vision_encoder.layers.9.attn.rel_pos_h\n",
      "vision_encoder.layers.9.attn.rel_pos_w\n",
      "vision_encoder.layers.9.attn.qkv.weight\n",
      "vision_encoder.layers.9.attn.qkv.bias\n",
      "vision_encoder.layers.9.attn.proj.weight\n",
      "vision_encoder.layers.9.attn.proj.bias\n",
      "vision_encoder.layers.9.layer_norm2.weight\n",
      "vision_encoder.layers.9.layer_norm2.bias\n",
      "vision_encoder.layers.9.mlp.lin1.weight\n",
      "vision_encoder.layers.9.mlp.lin1.bias\n",
      "vision_encoder.layers.9.mlp.lin2.weight\n",
      "vision_encoder.layers.9.mlp.lin2.bias\n",
      "vision_encoder.layers.10.layer_norm1.weight\n",
      "vision_encoder.layers.10.layer_norm1.bias\n",
      "vision_encoder.layers.10.attn.rel_pos_h\n",
      "vision_encoder.layers.10.attn.rel_pos_w\n",
      "vision_encoder.layers.10.attn.qkv.weight\n",
      "vision_encoder.layers.10.attn.qkv.bias\n",
      "vision_encoder.layers.10.attn.proj.weight\n",
      "vision_encoder.layers.10.attn.proj.bias\n",
      "vision_encoder.layers.10.layer_norm2.weight\n",
      "vision_encoder.layers.10.layer_norm2.bias\n",
      "vision_encoder.layers.10.mlp.lin1.weight\n",
      "vision_encoder.layers.10.mlp.lin1.bias\n",
      "vision_encoder.layers.10.mlp.lin2.weight\n",
      "vision_encoder.layers.10.mlp.lin2.bias\n",
      "vision_encoder.layers.11.layer_norm1.weight\n",
      "vision_encoder.layers.11.layer_norm1.bias\n",
      "vision_encoder.layers.11.attn.rel_pos_h\n",
      "vision_encoder.layers.11.attn.rel_pos_w\n",
      "vision_encoder.layers.11.attn.qkv.weight\n",
      "vision_encoder.layers.11.attn.qkv.bias\n",
      "vision_encoder.layers.11.attn.proj.weight\n",
      "vision_encoder.layers.11.attn.proj.bias\n",
      "vision_encoder.layers.11.layer_norm2.weight\n",
      "vision_encoder.layers.11.layer_norm2.bias\n",
      "vision_encoder.layers.11.mlp.lin1.weight\n",
      "vision_encoder.layers.11.mlp.lin1.bias\n",
      "vision_encoder.layers.11.mlp.lin2.weight\n",
      "vision_encoder.layers.11.mlp.lin2.bias\n",
      "vision_encoder.neck.conv1.weight\n",
      "vision_encoder.neck.layer_norm1.weight\n",
      "vision_encoder.neck.layer_norm1.bias\n",
      "vision_encoder.neck.conv2.weight\n",
      "vision_encoder.neck.layer_norm2.weight\n",
      "vision_encoder.neck.layer_norm2.bias\n",
      "prompt_encoder.mask_embed.conv1.weight\n",
      "prompt_encoder.mask_embed.conv1.bias\n",
      "prompt_encoder.mask_embed.conv2.weight\n",
      "prompt_encoder.mask_embed.conv2.bias\n",
      "prompt_encoder.mask_embed.conv3.weight\n",
      "prompt_encoder.mask_embed.conv3.bias\n",
      "prompt_encoder.mask_embed.layer_norm1.weight\n",
      "prompt_encoder.mask_embed.layer_norm1.bias\n",
      "prompt_encoder.mask_embed.layer_norm2.weight\n",
      "prompt_encoder.mask_embed.layer_norm2.bias\n",
      "prompt_encoder.no_mask_embed.weight\n",
      "prompt_encoder.point_embed.0.weight\n",
      "prompt_encoder.point_embed.1.weight\n",
      "prompt_encoder.point_embed.2.weight\n",
      "prompt_encoder.point_embed.3.weight\n",
      "prompt_encoder.not_a_point_embed.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm()\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load the pretrained weights for finetuning\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "# make sure we only compute gradients for mask decoder (encoder weights are frozen)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        print(name)\n",
    "        param.requires_grad_(False)   \n",
    "state_dict = torch.load(\"best_weights_v2.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images 790\n",
      "Number of validation images 301\n",
      "Number of test images 1208\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from monai.transforms import (Compose, LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, CenterSpatialCropd,\n",
    "                              ScaleIntensityRanged, ResizeD,SpatialPadd)\n",
    "# Initialize dictionary for storing image and label paths\n",
    "data_paths = {}\n",
    "base_dir = './dataset_512'\n",
    "datasets = ['train', 'val', 'test']\n",
    "data_types = ['2d_images', '2d_masks']\n",
    "# Create directories and print the number of images and masks in each\n",
    "for dataset in datasets:\n",
    "    for data_type in data_types:\n",
    "        # Construct the directory path\n",
    "        dir_path = os.path.join(base_dir, f'{dataset}_{data_type}_512')\n",
    "        \n",
    "        # Find images and labels in the directory\n",
    "        files = sorted(glob.glob(os.path.join(dir_path, \"*.nii.gz\")))\n",
    "        \n",
    "        # Store the image and label paths in the dictionary\n",
    "        data_paths[f'{dataset}_{data_type.split(\"_\")[1]}'] = files\n",
    "\n",
    "print('Number of training images', len(data_paths['train_images']))\n",
    "print('Number of validation images', len(data_paths['val_images']))\n",
    "print('Number of test images', len(data_paths['test_images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "    '''\n",
    "    This function creates varying bounding box coordinates based on the segmentation contours as prompt for the SAM model\n",
    "    The padding is random int values between 5 and 20 pixels\n",
    "    '''\n",
    "\n",
    "    if len(np.unique(ground_truth_map)) > 1:\n",
    "\n",
    "        # get bounding box from mask\n",
    "        y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        \n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = ground_truth_map.shape\n",
    "        x_min = max(0, x_min - np.random.randint(5, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(5, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(5, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(5, 20))\n",
    "        \n",
    "        bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        return bbox\n",
    "    else:\n",
    "        return [0, 0, 512, 512] # if there is no mask in the array, set bbox to image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset2(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, processor):\n",
    "        \n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.processor = processor\n",
    "        self.transforms = Compose([\n",
    "            LoadImaged(keys=['img', 'label']),\n",
    "            EnsureChannelFirstd(keys=['img', 'label']),\n",
    "            Orientationd(keys=['img', 'label'], axcodes='RA'),\n",
    "#             Spacingd(keys=['img', 'label'], pixdim=(1.5, 1.5), mode=(\"bilinear\", \"nearest\")),\n",
    "#             CenterSpatialCropd(keys=['img', 'label'], roi_size=(256,256)),\n",
    "            ResizeD(keys=['img', 'label'], spatial_size=(256,256), mode=(\"bilinear\", \"nearest\")),\n",
    "            ScaleIntensityRanged(keys=['img'], a_min=-1000, a_max=2000, \n",
    "                         b_min=0.0, b_max=255.0, clip=True), \n",
    "            ScaleIntensityRanged(keys=['label'], a_min=0, a_max=255, \n",
    "                         b_min=0.0, b_max=1.0, clip=True), \n",
    "#             SpatialPadd(keys=[\"img\", \"label\"], spatial_size=(256,256))\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        data_dict = self.transforms({'img': image_path, 'label': mask_path})\n",
    "\n",
    "        image = data_dict['img'].squeeze()\n",
    "        ground_truth_mask = data_dict['label'].squeeze()\n",
    "\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        array_rgb = np.dstack((image, image, image))\n",
    "        \n",
    "        image_rgb = Image.fromarray(array_rgb)\n",
    "        \n",
    "        ground_truth_mask[ground_truth_mask < 0] = 1\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        \n",
    "        inputs = self.processor(image_rgb, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        inputs[\"ground_truth_mask\"] = torch.from_numpy(ground_truth_mask.astype(np.int8))\n",
    "        inputs[\"image_name\"] = os.path.basename(image_path).replace('.nii.gz', '.png') # Change the extension\n",
    "\n",
    "        return inputs\n",
    "\n",
    "test_dataset = SAMDataset2(image_paths=data_paths['test_images'], mask_paths=data_paths['test_masks'], processor=processor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [04:29<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists('./dataset_512/infer_new_sam_b'):\n",
    "    os.makedirs('./dataset_512/infer_new_sam_b')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].cuda(1),\n",
    "                        input_boxes=batch[\"input_boxes\"].cuda(1),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "        medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "        labeled, num_features = label(medsam_seg)\n",
    "        border_mask = np.zeros_like(medsam_seg)\n",
    "        border_mask[:, 0] = 1\n",
    "        border_mask[0, :] = 1\n",
    "        border_mask[:, -1] = 1\n",
    "        border_mask[-1, :] = 1\n",
    "\n",
    "        touching_border = np.unique(labeled * binary_dilation(border_mask))\n",
    "        for region in touching_border:\n",
    "            if region != 0:\n",
    "                medsam_seg[labeled == region] = 0\n",
    "        for i in range(1, num_features + 1):\n",
    "            area = ndi_sum(labeled == i)\n",
    "            if area <= 4:\n",
    "                medsam_seg[labeled == i] = 0\n",
    "        original_image = batch[\"pixel_values\"][0, 1].cpu().numpy().astype(np.uint8)\n",
    "        # print(original_image.shape)\n",
    "        edges_original = cv2.Canny(original_image, 50, 200)\n",
    "        \n",
    "        kernel = np.ones((3,3),np.uint8)\n",
    "        dilated = cv2.dilate(edges_original, kernel, iterations=1)\n",
    "        contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        mask = np.zeros_like(edges_original)\n",
    "        # print(mask.shape)\n",
    "        # 在mask上画出最大的轮廓\n",
    "        cv2.drawContours(mask, [max_contour],-1, (255), thickness=cv2.FILLED)\n",
    "        # 创建一个比输入图像大2的掩码，用于floodFill函数\n",
    "        h, w = mask.shape[:2]\n",
    "        mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
    "        # floodFill函数会改变输入图像，所以我们使用它的副本\n",
    "        mask_floodfill_copy = mask.copy()\n",
    "        # 找到一个种子点\n",
    "        seed_point = (w//2, h//2)\n",
    "        # 执行floodFill函数，将与种子点连通的区域填充为白色\n",
    "        cv2.floodFill(mask_floodfill_copy, mask_floodfill, seed_point, 255)\n",
    "        final_mask = mask | mask_floodfill_copy\n",
    "        final_mask = cv2.resize(final_mask, (256, 256))\n",
    "        # print(final_mask.shape)\n",
    "        # print(medsam_seg.shape)\n",
    "       \n",
    "        masked_seg = cv2.bitwise_or(medsam_seg,medsam_seg, mask=final_mask)\n",
    "        final_mask = cv2.resize(masked_seg, (512, 512))\n",
    "        closed_seg = cv2.morphologyEx(masked_seg, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        result_image_name = os.path.join('./dataset_512/infer_new_sam_b', batch[\"image_name\"][0])\n",
    "        result_image = Image.fromarray((closed_seg * 255).astype(np.uint8))\n",
    "        result_image.save(result_image_name)\n",
    "\n",
    "        # plt.figure(figsize=(12,4))\n",
    "        # plt.subplot(1,3,1)\n",
    "        # plt.imshow(batch[\"pixel_values\"][0,1], cmap='gray')\n",
    "        # plt.title('original_image')\n",
    "        # plt.axis('off')\n",
    "        # plt.subplot(1,3,2)\n",
    "        # plt.imshow(batch[\"ground_truth_mask\"][0], cmap='copper')\n",
    "        # plt.title('ground_truth_masks')\n",
    "        # plt.axis('off')\n",
    "        # plt.subplot(1,3,3)\n",
    "        # plt.imshow(closed_seg, cmap='copper')\n",
    "        # plt.title('after_canny')\n",
    "        # plt.axis('off')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(result_image_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input array: (512, 512)\n",
      "lungmask 2024-01-08 11:23:34 Postprocessing\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from lungmask import LMInferer\n",
    "import SimpleITK as sitk\n",
    "from monai.transforms import Compose, EnsureChannelFirst, Orientation, Resize, ScaleIntensityRange,LoadImaged\n",
    "\n",
    "inferer = LMInferer(tqdm_disable=True)\n",
    "\n",
    "input_image = sitk.ReadImage(\"./dataset_512/test_2d_images_512/IMG_0002_65.nii.gz\")\n",
    "# input_image = sitk.Cast(input_image, sitk.sitkFloat32)\n",
    "input_array = sitk.GetArrayFromImage(input_image)\n",
    "# input=LoadImaged(\"./dataset_512/test_2d_images_512/IMG_0002_65.nii.gz\")\n",
    "# transformations = Compose([\n",
    "#     # EnsureChannelFirst(),\n",
    "#     Orientation(axcodes='RA'),\n",
    "#     Resize(spatial_size=(256,256), mode=\"bilinear\"),\n",
    "#     ScaleIntensityRange(a_min=-1000, a_max=2000, b_min=0.0, b_max=255.0, clip=True)\n",
    "# ])\n",
    "# # Apply the transformations\n",
    "# input_array = transformations(input_array)\n",
    "# Check the shape of the array\n",
    "print(f\"Shape of the input array: {input_array.shape}\")\n",
    "\n",
    "# volume_3d_image = sitk.GetImageFromArray(input_array)\n",
    "\n",
    "#     # Now you can apply the lungmask package to the 3D image\n",
    "# segmentation = inferer.apply(volume_3d_image)\n",
    "\n",
    "# # If the input array is 4D (e.g., (time, z, y, x)), select a 3D volume\n",
    "# if len(input_array.shape) == 4:\n",
    "#     # Select the first time point as an example\n",
    "#     volume_3d = input_array[0]\n",
    "if len(input_array.shape)==2:\n",
    "    volume_3d=input_array[np.newaxis,:,:]\n",
    "    # Convert the 3D volume back to a SimpleITK image\n",
    "    volume_3d_image = sitk.GetImageFromArray(volume_3d)\n",
    "    # Now you can apply the lungmask package to the 3D image\n",
    "    segmentation = inferer.apply(volume_3d_image)\n",
    "\n",
    "segmentation = segmentation.transpose((2, 1, 0)) \n",
    "corrected_segmentation = np.fliplr(segmentation)\n",
    "# slice_index=100\n",
    "# result_image_name_lungmask = os.path.join(output_dir, batch[\"image_name\"][0])\n",
    "# plt.imshow(segmentation, cmap='gray')\n",
    "# print(corrected_segmentation[300])\n",
    "normalized_volume = ((volume_3d - volume_3d.min()) / (volume_3d.max() - volume_3d.min()) * 255).astype(np.uint8)\n",
    "edges_original = cv2.Canny(normalized_volume, 50, 200)\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "dilated = cv2.dilate(edges_original, kernel, iterations=1)\n",
    "contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "max_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "mask = np.zeros_like(edges_original)\n",
    "# 在mask上画出最大的轮廓\n",
    "cv2.drawContours(mask, [max_contour],-1, (255), thickness=cv2.FILLED)\n",
    "# 创建一个比输入图像大2的掩码，用于floodFill函数\n",
    "h, w = mask.shape[:2]\n",
    "mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
    "# floodFill函数会改变输入图像，所以我们使用它的副本\n",
    "mask_floodfill_copy = mask.copy()\n",
    "# 找到一个种子点\n",
    "seed_point = (w//2, h//2)\n",
    "# 执行floodFill函数，将与种子点连通的区域填充为白色\n",
    "cv2.floodFill(mask_floodfill_copy, mask_floodfill, seed_point, 255)\n",
    "final_mask = mask | mask_floodfill_copy\n",
    "final_mask = cv2.resize(final_mask, (256, 256))\n",
    "\n",
    "# Ensure both images have the same size\n",
    "final_mask = cv2.resize(final_mask, (corrected_segmentation.shape[1], corrected_segmentation.shape[0]))\n",
    "\n",
    "# Ensure both images have the same data type (8-bit unsigned integer)\n",
    "final_mask = final_mask.astype(np.uint8)\n",
    "masked_seg = cv2.bitwise_or(corrected_segmentation,corrected_segmentation, mask=final_mask)\n",
    "closed_seg = cv2.morphologyEx(masked_seg, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "white_mask = closed_seg > 0\n",
    "closed_seg[white_mask] = 255\n",
    "\n",
    "plt.imshow(closed_seg,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# print(closed_seg)\n",
    "# # print(input_image)\n",
    "# # Apply lung segmentation using lungmask\n",
    "# segmentation = inferer.apply(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [5:45:25<00:00, 17.16s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lungmask import LMInferer\n",
    "import SimpleITK as sitk\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Create the lungmask inferer\n",
    "inferer = LMInferer(modelname='LTRCLobes', fillmodel='R231',tqdm_disable=True)\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = \"./dataset_512/test_2d_images_512/\"\n",
    "output_dir = \"./dataset_512/infer_new_lungmask\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List of test file paths\n",
    "test_files = [os.path.join(input_dir, filename) for filename in os.listdir(input_dir) if filename.endswith('.nii.gz')]\n",
    "\n",
    "# Perform inference for each test file\n",
    "for test_file in tqdm(test_files):\n",
    "    # Read the NIfTI file using SimpleITK\n",
    "    input_image = sitk.ReadImage(test_file)\n",
    "    input_array = sitk.GetArrayFromImage(input_image)\n",
    "\n",
    "    if len(input_array.shape)==2:\n",
    "        volume_3d=input_array[np.newaxis,:,:]\n",
    "        # Convert the 3D volume back to a SimpleITK image\n",
    "        volume_3d_image = sitk.GetImageFromArray(volume_3d)\n",
    "        # Now you can apply the lungmask package to the 3D image\n",
    "        segmentation = inferer.apply(volume_3d_image)\n",
    "\n",
    "    segmentation = segmentation.transpose((2, 1, 0)) \n",
    "    corrected_segmentation = np.fliplr(segmentation)\n",
    "    \n",
    "    white_mask = corrected_segmentation > 0\n",
    "    corrected_segmentation[white_mask] = 255\n",
    "\n",
    "    normalized_volume = ((volume_3d - volume_3d.min()) / (volume_3d.max() - volume_3d.min()) * 255).astype(np.uint8)\n",
    "    edges_original = cv2.Canny(normalized_volume, 50, 200)\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    dilated = cv2.dilate(edges_original, kernel, iterations=1)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    mask = np.zeros_like(edges_original)\n",
    "    # 在mask上画出最大的轮廓\n",
    "    cv2.drawContours(mask, [max_contour],-1, (255), thickness=cv2.FILLED)\n",
    "    # 创建一个比输入图像大2的掩码，用于floodFill函数\n",
    "    h, w = mask.shape[:2]\n",
    "    mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
    "    # floodFill函数会改变输入图像，所以我们使用它的副本\n",
    "    mask_floodfill_copy = mask.copy()\n",
    "    # 找到一个种子点\n",
    "    seed_point = (w//2, h//2)\n",
    "    # 执行floodFill函数，将与种子点连通的区域填充为白色\n",
    "    cv2.floodFill(mask_floodfill_copy, mask_floodfill, seed_point, 255)\n",
    "    final_mask = mask | mask_floodfill_copy\n",
    "    final_mask = cv2.resize(final_mask, (256, 256))\n",
    "\n",
    "    # Ensure both images have the same size\n",
    "    final_mask = cv2.resize(final_mask, (corrected_segmentation.shape[1], corrected_segmentation.shape[0]))\n",
    "\n",
    "    # Ensure both images have the same data type (8-bit unsigned integer)\n",
    "    final_mask = final_mask.astype(np.uint8)\n",
    "    masked_seg = cv2.bitwise_or(corrected_segmentation,corrected_segmentation, mask=final_mask)\n",
    "    closed_seg = cv2.morphologyEx(masked_seg, cv2.MORPH_CLOSE, kernel)\n",
    "    # Save the result\n",
    "    # print(test_file)\n",
    "    # break\n",
    "    png_filename = os.path.basename(test_file).rsplit('.', 2)[0] + '.png'\n",
    "    result_image_path = os.path.join(output_dir, png_filename)\n",
    "    plt.imshow(closed_seg, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(result_image_path)\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lungmask import LMInferer\n",
    "# import SimpleITK as sitk\n",
    "\n",
    "# # Create the lungmask inferer\n",
    "# inferer = LMInferer(tqdm_disable=True)\n",
    "\n",
    "# # Define the input and output directories\n",
    "# input_dir = \"./dataset_512/test_2d_images_512/\"\n",
    "# output_dir = \"./dataset_512/infer_new_lungmask\"\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # # List of test file paths\n",
    "# # test_files = [os.path.join(input_dir, filename) for filename in os.listdir(input_dir) if filename.endswith('.nii.gz')]\n",
    "\n",
    "# # Perform inference for each test file\n",
    "# # for test_file in tqdm(test_files):\n",
    "# with torch.no_grad():\n",
    "#     # Read the NIfTI file using SimpleITK\n",
    "#    for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "#         # print(batch)\n",
    "#         # image_rgb = batch[\"pixel_values\"].numpy().squeeze().astype(np.uint8)\n",
    "#         original_image = batch[\"pixel_values\"][0].squeeze().numpy().astype(np.uint8)\n",
    "#         print(original_image.shape)\n",
    "#         # # Apply lung segmentation using lungmask\n",
    "#         # lung_mask = inferer.apply(image_rgb)\n",
    "\n",
    "#         # # Convert the SimpleITK segmentation to NumPy array\n",
    "#         # lung_mask_np = sitk.GetArrayFromImage(lung_mask)\n",
    "#         lung_mask_np=inferer.apply(original_image)\n",
    "#         # print(type(lung_mask_np))\n",
    "#         lung_mask_np = lung_mask_np.transpose((1, 2, 0))  # If lung_mask_np is a numpy array\n",
    "#         # Additional post-processing steps if needed\n",
    "#         # ...\n",
    "\n",
    "#         # Save the result\n",
    "#         result_image_name_lungmask = os.path.join(output_dir, batch[\"image_name\"][0])\n",
    "#         plt.imshow(lung_mask_np, cmap='gray')\n",
    "#         plt.axis('off')\n",
    "#         plt.savefig(result_image_name_lungmask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
