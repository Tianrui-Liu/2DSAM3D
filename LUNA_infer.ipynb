{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images 49823\n",
      "Number of validation images 6623\n",
      "Number of test images 134456\n",
      "vision_encoder.pos_embed\n",
      "vision_encoder.patch_embed.projection.weight\n",
      "vision_encoder.patch_embed.projection.bias\n",
      "vision_encoder.layers.0.layer_norm1.weight\n",
      "vision_encoder.layers.0.layer_norm1.bias\n",
      "vision_encoder.layers.0.attn.rel_pos_h\n",
      "vision_encoder.layers.0.attn.rel_pos_w\n",
      "vision_encoder.layers.0.attn.qkv.weight\n",
      "vision_encoder.layers.0.attn.qkv.bias\n",
      "vision_encoder.layers.0.attn.proj.weight\n",
      "vision_encoder.layers.0.attn.proj.bias\n",
      "vision_encoder.layers.0.layer_norm2.weight\n",
      "vision_encoder.layers.0.layer_norm2.bias\n",
      "vision_encoder.layers.0.mlp.lin1.weight\n",
      "vision_encoder.layers.0.mlp.lin1.bias\n",
      "vision_encoder.layers.0.mlp.lin2.weight\n",
      "vision_encoder.layers.0.mlp.lin2.bias\n",
      "vision_encoder.layers.1.layer_norm1.weight\n",
      "vision_encoder.layers.1.layer_norm1.bias\n",
      "vision_encoder.layers.1.attn.rel_pos_h\n",
      "vision_encoder.layers.1.attn.rel_pos_w\n",
      "vision_encoder.layers.1.attn.qkv.weight\n",
      "vision_encoder.layers.1.attn.qkv.bias\n",
      "vision_encoder.layers.1.attn.proj.weight\n",
      "vision_encoder.layers.1.attn.proj.bias\n",
      "vision_encoder.layers.1.layer_norm2.weight\n",
      "vision_encoder.layers.1.layer_norm2.bias\n",
      "vision_encoder.layers.1.mlp.lin1.weight\n",
      "vision_encoder.layers.1.mlp.lin1.bias\n",
      "vision_encoder.layers.1.mlp.lin2.weight\n",
      "vision_encoder.layers.1.mlp.lin2.bias\n",
      "vision_encoder.layers.2.layer_norm1.weight\n",
      "vision_encoder.layers.2.layer_norm1.bias\n",
      "vision_encoder.layers.2.attn.rel_pos_h\n",
      "vision_encoder.layers.2.attn.rel_pos_w\n",
      "vision_encoder.layers.2.attn.qkv.weight\n",
      "vision_encoder.layers.2.attn.qkv.bias\n",
      "vision_encoder.layers.2.attn.proj.weight\n",
      "vision_encoder.layers.2.attn.proj.bias\n",
      "vision_encoder.layers.2.layer_norm2.weight\n",
      "vision_encoder.layers.2.layer_norm2.bias\n",
      "vision_encoder.layers.2.mlp.lin1.weight\n",
      "vision_encoder.layers.2.mlp.lin1.bias\n",
      "vision_encoder.layers.2.mlp.lin2.weight\n",
      "vision_encoder.layers.2.mlp.lin2.bias\n",
      "vision_encoder.layers.3.layer_norm1.weight\n",
      "vision_encoder.layers.3.layer_norm1.bias\n",
      "vision_encoder.layers.3.attn.rel_pos_h\n",
      "vision_encoder.layers.3.attn.rel_pos_w\n",
      "vision_encoder.layers.3.attn.qkv.weight\n",
      "vision_encoder.layers.3.attn.qkv.bias\n",
      "vision_encoder.layers.3.attn.proj.weight\n",
      "vision_encoder.layers.3.attn.proj.bias\n",
      "vision_encoder.layers.3.layer_norm2.weight\n",
      "vision_encoder.layers.3.layer_norm2.bias\n",
      "vision_encoder.layers.3.mlp.lin1.weight\n",
      "vision_encoder.layers.3.mlp.lin1.bias\n",
      "vision_encoder.layers.3.mlp.lin2.weight\n",
      "vision_encoder.layers.3.mlp.lin2.bias\n",
      "vision_encoder.layers.4.layer_norm1.weight\n",
      "vision_encoder.layers.4.layer_norm1.bias\n",
      "vision_encoder.layers.4.attn.rel_pos_h\n",
      "vision_encoder.layers.4.attn.rel_pos_w\n",
      "vision_encoder.layers.4.attn.qkv.weight\n",
      "vision_encoder.layers.4.attn.qkv.bias\n",
      "vision_encoder.layers.4.attn.proj.weight\n",
      "vision_encoder.layers.4.attn.proj.bias\n",
      "vision_encoder.layers.4.layer_norm2.weight\n",
      "vision_encoder.layers.4.layer_norm2.bias\n",
      "vision_encoder.layers.4.mlp.lin1.weight\n",
      "vision_encoder.layers.4.mlp.lin1.bias\n",
      "vision_encoder.layers.4.mlp.lin2.weight\n",
      "vision_encoder.layers.4.mlp.lin2.bias\n",
      "vision_encoder.layers.5.layer_norm1.weight\n",
      "vision_encoder.layers.5.layer_norm1.bias\n",
      "vision_encoder.layers.5.attn.rel_pos_h\n",
      "vision_encoder.layers.5.attn.rel_pos_w\n",
      "vision_encoder.layers.5.attn.qkv.weight\n",
      "vision_encoder.layers.5.attn.qkv.bias\n",
      "vision_encoder.layers.5.attn.proj.weight\n",
      "vision_encoder.layers.5.attn.proj.bias\n",
      "vision_encoder.layers.5.layer_norm2.weight\n",
      "vision_encoder.layers.5.layer_norm2.bias\n",
      "vision_encoder.layers.5.mlp.lin1.weight\n",
      "vision_encoder.layers.5.mlp.lin1.bias\n",
      "vision_encoder.layers.5.mlp.lin2.weight\n",
      "vision_encoder.layers.5.mlp.lin2.bias\n",
      "vision_encoder.layers.6.layer_norm1.weight\n",
      "vision_encoder.layers.6.layer_norm1.bias\n",
      "vision_encoder.layers.6.attn.rel_pos_h\n",
      "vision_encoder.layers.6.attn.rel_pos_w\n",
      "vision_encoder.layers.6.attn.qkv.weight\n",
      "vision_encoder.layers.6.attn.qkv.bias\n",
      "vision_encoder.layers.6.attn.proj.weight\n",
      "vision_encoder.layers.6.attn.proj.bias\n",
      "vision_encoder.layers.6.layer_norm2.weight\n",
      "vision_encoder.layers.6.layer_norm2.bias\n",
      "vision_encoder.layers.6.mlp.lin1.weight\n",
      "vision_encoder.layers.6.mlp.lin1.bias\n",
      "vision_encoder.layers.6.mlp.lin2.weight\n",
      "vision_encoder.layers.6.mlp.lin2.bias\n",
      "vision_encoder.layers.7.layer_norm1.weight\n",
      "vision_encoder.layers.7.layer_norm1.bias\n",
      "vision_encoder.layers.7.attn.rel_pos_h\n",
      "vision_encoder.layers.7.attn.rel_pos_w\n",
      "vision_encoder.layers.7.attn.qkv.weight\n",
      "vision_encoder.layers.7.attn.qkv.bias\n",
      "vision_encoder.layers.7.attn.proj.weight\n",
      "vision_encoder.layers.7.attn.proj.bias\n",
      "vision_encoder.layers.7.layer_norm2.weight\n",
      "vision_encoder.layers.7.layer_norm2.bias\n",
      "vision_encoder.layers.7.mlp.lin1.weight\n",
      "vision_encoder.layers.7.mlp.lin1.bias\n",
      "vision_encoder.layers.7.mlp.lin2.weight\n",
      "vision_encoder.layers.7.mlp.lin2.bias\n",
      "vision_encoder.layers.8.layer_norm1.weight\n",
      "vision_encoder.layers.8.layer_norm1.bias\n",
      "vision_encoder.layers.8.attn.rel_pos_h\n",
      "vision_encoder.layers.8.attn.rel_pos_w\n",
      "vision_encoder.layers.8.attn.qkv.weight\n",
      "vision_encoder.layers.8.attn.qkv.bias\n",
      "vision_encoder.layers.8.attn.proj.weight\n",
      "vision_encoder.layers.8.attn.proj.bias\n",
      "vision_encoder.layers.8.layer_norm2.weight\n",
      "vision_encoder.layers.8.layer_norm2.bias\n",
      "vision_encoder.layers.8.mlp.lin1.weight\n",
      "vision_encoder.layers.8.mlp.lin1.bias\n",
      "vision_encoder.layers.8.mlp.lin2.weight\n",
      "vision_encoder.layers.8.mlp.lin2.bias\n",
      "vision_encoder.layers.9.layer_norm1.weight\n",
      "vision_encoder.layers.9.layer_norm1.bias\n",
      "vision_encoder.layers.9.attn.rel_pos_h\n",
      "vision_encoder.layers.9.attn.rel_pos_w\n",
      "vision_encoder.layers.9.attn.qkv.weight\n",
      "vision_encoder.layers.9.attn.qkv.bias\n",
      "vision_encoder.layers.9.attn.proj.weight\n",
      "vision_encoder.layers.9.attn.proj.bias\n",
      "vision_encoder.layers.9.layer_norm2.weight\n",
      "vision_encoder.layers.9.layer_norm2.bias\n",
      "vision_encoder.layers.9.mlp.lin1.weight\n",
      "vision_encoder.layers.9.mlp.lin1.bias\n",
      "vision_encoder.layers.9.mlp.lin2.weight\n",
      "vision_encoder.layers.9.mlp.lin2.bias\n",
      "vision_encoder.layers.10.layer_norm1.weight\n",
      "vision_encoder.layers.10.layer_norm1.bias\n",
      "vision_encoder.layers.10.attn.rel_pos_h\n",
      "vision_encoder.layers.10.attn.rel_pos_w\n",
      "vision_encoder.layers.10.attn.qkv.weight\n",
      "vision_encoder.layers.10.attn.qkv.bias\n",
      "vision_encoder.layers.10.attn.proj.weight\n",
      "vision_encoder.layers.10.attn.proj.bias\n",
      "vision_encoder.layers.10.layer_norm2.weight\n",
      "vision_encoder.layers.10.layer_norm2.bias\n",
      "vision_encoder.layers.10.mlp.lin1.weight\n",
      "vision_encoder.layers.10.mlp.lin1.bias\n",
      "vision_encoder.layers.10.mlp.lin2.weight\n",
      "vision_encoder.layers.10.mlp.lin2.bias\n",
      "vision_encoder.layers.11.layer_norm1.weight\n",
      "vision_encoder.layers.11.layer_norm1.bias\n",
      "vision_encoder.layers.11.attn.rel_pos_h\n",
      "vision_encoder.layers.11.attn.rel_pos_w\n",
      "vision_encoder.layers.11.attn.qkv.weight\n",
      "vision_encoder.layers.11.attn.qkv.bias\n",
      "vision_encoder.layers.11.attn.proj.weight\n",
      "vision_encoder.layers.11.attn.proj.bias\n",
      "vision_encoder.layers.11.layer_norm2.weight\n",
      "vision_encoder.layers.11.layer_norm2.bias\n",
      "vision_encoder.layers.11.mlp.lin1.weight\n",
      "vision_encoder.layers.11.mlp.lin1.bias\n",
      "vision_encoder.layers.11.mlp.lin2.weight\n",
      "vision_encoder.layers.11.mlp.lin2.bias\n",
      "vision_encoder.layers.12.layer_norm1.weight\n",
      "vision_encoder.layers.12.layer_norm1.bias\n",
      "vision_encoder.layers.12.attn.rel_pos_h\n",
      "vision_encoder.layers.12.attn.rel_pos_w\n",
      "vision_encoder.layers.12.attn.qkv.weight\n",
      "vision_encoder.layers.12.attn.qkv.bias\n",
      "vision_encoder.layers.12.attn.proj.weight\n",
      "vision_encoder.layers.12.attn.proj.bias\n",
      "vision_encoder.layers.12.layer_norm2.weight\n",
      "vision_encoder.layers.12.layer_norm2.bias\n",
      "vision_encoder.layers.12.mlp.lin1.weight\n",
      "vision_encoder.layers.12.mlp.lin1.bias\n",
      "vision_encoder.layers.12.mlp.lin2.weight\n",
      "vision_encoder.layers.12.mlp.lin2.bias\n",
      "vision_encoder.layers.13.layer_norm1.weight\n",
      "vision_encoder.layers.13.layer_norm1.bias\n",
      "vision_encoder.layers.13.attn.rel_pos_h\n",
      "vision_encoder.layers.13.attn.rel_pos_w\n",
      "vision_encoder.layers.13.attn.qkv.weight\n",
      "vision_encoder.layers.13.attn.qkv.bias\n",
      "vision_encoder.layers.13.attn.proj.weight\n",
      "vision_encoder.layers.13.attn.proj.bias\n",
      "vision_encoder.layers.13.layer_norm2.weight\n",
      "vision_encoder.layers.13.layer_norm2.bias\n",
      "vision_encoder.layers.13.mlp.lin1.weight\n",
      "vision_encoder.layers.13.mlp.lin1.bias\n",
      "vision_encoder.layers.13.mlp.lin2.weight\n",
      "vision_encoder.layers.13.mlp.lin2.bias\n",
      "vision_encoder.layers.14.layer_norm1.weight\n",
      "vision_encoder.layers.14.layer_norm1.bias\n",
      "vision_encoder.layers.14.attn.rel_pos_h\n",
      "vision_encoder.layers.14.attn.rel_pos_w\n",
      "vision_encoder.layers.14.attn.qkv.weight\n",
      "vision_encoder.layers.14.attn.qkv.bias\n",
      "vision_encoder.layers.14.attn.proj.weight\n",
      "vision_encoder.layers.14.attn.proj.bias\n",
      "vision_encoder.layers.14.layer_norm2.weight\n",
      "vision_encoder.layers.14.layer_norm2.bias\n",
      "vision_encoder.layers.14.mlp.lin1.weight\n",
      "vision_encoder.layers.14.mlp.lin1.bias\n",
      "vision_encoder.layers.14.mlp.lin2.weight\n",
      "vision_encoder.layers.14.mlp.lin2.bias\n",
      "vision_encoder.layers.15.layer_norm1.weight\n",
      "vision_encoder.layers.15.layer_norm1.bias\n",
      "vision_encoder.layers.15.attn.rel_pos_h\n",
      "vision_encoder.layers.15.attn.rel_pos_w\n",
      "vision_encoder.layers.15.attn.qkv.weight\n",
      "vision_encoder.layers.15.attn.qkv.bias\n",
      "vision_encoder.layers.15.attn.proj.weight\n",
      "vision_encoder.layers.15.attn.proj.bias\n",
      "vision_encoder.layers.15.layer_norm2.weight\n",
      "vision_encoder.layers.15.layer_norm2.bias\n",
      "vision_encoder.layers.15.mlp.lin1.weight\n",
      "vision_encoder.layers.15.mlp.lin1.bias\n",
      "vision_encoder.layers.15.mlp.lin2.weight\n",
      "vision_encoder.layers.15.mlp.lin2.bias\n",
      "vision_encoder.layers.16.layer_norm1.weight\n",
      "vision_encoder.layers.16.layer_norm1.bias\n",
      "vision_encoder.layers.16.attn.rel_pos_h\n",
      "vision_encoder.layers.16.attn.rel_pos_w\n",
      "vision_encoder.layers.16.attn.qkv.weight\n",
      "vision_encoder.layers.16.attn.qkv.bias\n",
      "vision_encoder.layers.16.attn.proj.weight\n",
      "vision_encoder.layers.16.attn.proj.bias\n",
      "vision_encoder.layers.16.layer_norm2.weight\n",
      "vision_encoder.layers.16.layer_norm2.bias\n",
      "vision_encoder.layers.16.mlp.lin1.weight\n",
      "vision_encoder.layers.16.mlp.lin1.bias\n",
      "vision_encoder.layers.16.mlp.lin2.weight\n",
      "vision_encoder.layers.16.mlp.lin2.bias\n",
      "vision_encoder.layers.17.layer_norm1.weight\n",
      "vision_encoder.layers.17.layer_norm1.bias\n",
      "vision_encoder.layers.17.attn.rel_pos_h\n",
      "vision_encoder.layers.17.attn.rel_pos_w\n",
      "vision_encoder.layers.17.attn.qkv.weight\n",
      "vision_encoder.layers.17.attn.qkv.bias\n",
      "vision_encoder.layers.17.attn.proj.weight\n",
      "vision_encoder.layers.17.attn.proj.bias\n",
      "vision_encoder.layers.17.layer_norm2.weight\n",
      "vision_encoder.layers.17.layer_norm2.bias\n",
      "vision_encoder.layers.17.mlp.lin1.weight\n",
      "vision_encoder.layers.17.mlp.lin1.bias\n",
      "vision_encoder.layers.17.mlp.lin2.weight\n",
      "vision_encoder.layers.17.mlp.lin2.bias\n",
      "vision_encoder.layers.18.layer_norm1.weight\n",
      "vision_encoder.layers.18.layer_norm1.bias\n",
      "vision_encoder.layers.18.attn.rel_pos_h\n",
      "vision_encoder.layers.18.attn.rel_pos_w\n",
      "vision_encoder.layers.18.attn.qkv.weight\n",
      "vision_encoder.layers.18.attn.qkv.bias\n",
      "vision_encoder.layers.18.attn.proj.weight\n",
      "vision_encoder.layers.18.attn.proj.bias\n",
      "vision_encoder.layers.18.layer_norm2.weight\n",
      "vision_encoder.layers.18.layer_norm2.bias\n",
      "vision_encoder.layers.18.mlp.lin1.weight\n",
      "vision_encoder.layers.18.mlp.lin1.bias\n",
      "vision_encoder.layers.18.mlp.lin2.weight\n",
      "vision_encoder.layers.18.mlp.lin2.bias\n",
      "vision_encoder.layers.19.layer_norm1.weight\n",
      "vision_encoder.layers.19.layer_norm1.bias\n",
      "vision_encoder.layers.19.attn.rel_pos_h\n",
      "vision_encoder.layers.19.attn.rel_pos_w\n",
      "vision_encoder.layers.19.attn.qkv.weight\n",
      "vision_encoder.layers.19.attn.qkv.bias\n",
      "vision_encoder.layers.19.attn.proj.weight\n",
      "vision_encoder.layers.19.attn.proj.bias\n",
      "vision_encoder.layers.19.layer_norm2.weight\n",
      "vision_encoder.layers.19.layer_norm2.bias\n",
      "vision_encoder.layers.19.mlp.lin1.weight\n",
      "vision_encoder.layers.19.mlp.lin1.bias\n",
      "vision_encoder.layers.19.mlp.lin2.weight\n",
      "vision_encoder.layers.19.mlp.lin2.bias\n",
      "vision_encoder.layers.20.layer_norm1.weight\n",
      "vision_encoder.layers.20.layer_norm1.bias\n",
      "vision_encoder.layers.20.attn.rel_pos_h\n",
      "vision_encoder.layers.20.attn.rel_pos_w\n",
      "vision_encoder.layers.20.attn.qkv.weight\n",
      "vision_encoder.layers.20.attn.qkv.bias\n",
      "vision_encoder.layers.20.attn.proj.weight\n",
      "vision_encoder.layers.20.attn.proj.bias\n",
      "vision_encoder.layers.20.layer_norm2.weight\n",
      "vision_encoder.layers.20.layer_norm2.bias\n",
      "vision_encoder.layers.20.mlp.lin1.weight\n",
      "vision_encoder.layers.20.mlp.lin1.bias\n",
      "vision_encoder.layers.20.mlp.lin2.weight\n",
      "vision_encoder.layers.20.mlp.lin2.bias\n",
      "vision_encoder.layers.21.layer_norm1.weight\n",
      "vision_encoder.layers.21.layer_norm1.bias\n",
      "vision_encoder.layers.21.attn.rel_pos_h\n",
      "vision_encoder.layers.21.attn.rel_pos_w\n",
      "vision_encoder.layers.21.attn.qkv.weight\n",
      "vision_encoder.layers.21.attn.qkv.bias\n",
      "vision_encoder.layers.21.attn.proj.weight\n",
      "vision_encoder.layers.21.attn.proj.bias\n",
      "vision_encoder.layers.21.layer_norm2.weight\n",
      "vision_encoder.layers.21.layer_norm2.bias\n",
      "vision_encoder.layers.21.mlp.lin1.weight\n",
      "vision_encoder.layers.21.mlp.lin1.bias\n",
      "vision_encoder.layers.21.mlp.lin2.weight\n",
      "vision_encoder.layers.21.mlp.lin2.bias\n",
      "vision_encoder.layers.22.layer_norm1.weight\n",
      "vision_encoder.layers.22.layer_norm1.bias\n",
      "vision_encoder.layers.22.attn.rel_pos_h\n",
      "vision_encoder.layers.22.attn.rel_pos_w\n",
      "vision_encoder.layers.22.attn.qkv.weight\n",
      "vision_encoder.layers.22.attn.qkv.bias\n",
      "vision_encoder.layers.22.attn.proj.weight\n",
      "vision_encoder.layers.22.attn.proj.bias\n",
      "vision_encoder.layers.22.layer_norm2.weight\n",
      "vision_encoder.layers.22.layer_norm2.bias\n",
      "vision_encoder.layers.22.mlp.lin1.weight\n",
      "vision_encoder.layers.22.mlp.lin1.bias\n",
      "vision_encoder.layers.22.mlp.lin2.weight\n",
      "vision_encoder.layers.22.mlp.lin2.bias\n",
      "vision_encoder.layers.23.layer_norm1.weight\n",
      "vision_encoder.layers.23.layer_norm1.bias\n",
      "vision_encoder.layers.23.attn.rel_pos_h\n",
      "vision_encoder.layers.23.attn.rel_pos_w\n",
      "vision_encoder.layers.23.attn.qkv.weight\n",
      "vision_encoder.layers.23.attn.qkv.bias\n",
      "vision_encoder.layers.23.attn.proj.weight\n",
      "vision_encoder.layers.23.attn.proj.bias\n",
      "vision_encoder.layers.23.layer_norm2.weight\n",
      "vision_encoder.layers.23.layer_norm2.bias\n",
      "vision_encoder.layers.23.mlp.lin1.weight\n",
      "vision_encoder.layers.23.mlp.lin1.bias\n",
      "vision_encoder.layers.23.mlp.lin2.weight\n",
      "vision_encoder.layers.23.mlp.lin2.bias\n",
      "vision_encoder.neck.conv1.weight\n",
      "vision_encoder.neck.layer_norm1.weight\n",
      "vision_encoder.neck.layer_norm1.bias\n",
      "vision_encoder.neck.conv2.weight\n",
      "vision_encoder.neck.layer_norm2.weight\n",
      "vision_encoder.neck.layer_norm2.bias\n",
      "prompt_encoder.mask_embed.conv1.weight\n",
      "prompt_encoder.mask_embed.conv1.bias\n",
      "prompt_encoder.mask_embed.conv2.weight\n",
      "prompt_encoder.mask_embed.conv2.bias\n",
      "prompt_encoder.mask_embed.conv3.weight\n",
      "prompt_encoder.mask_embed.conv3.bias\n",
      "prompt_encoder.mask_embed.layer_norm1.weight\n",
      "prompt_encoder.mask_embed.layer_norm1.bias\n",
      "prompt_encoder.mask_embed.layer_norm2.weight\n",
      "prompt_encoder.mask_embed.layer_norm2.bias\n",
      "prompt_encoder.no_mask_embed.weight\n",
      "prompt_encoder.point_embed.0.weight\n",
      "prompt_encoder.point_embed.1.weight\n",
      "prompt_encoder.point_embed.2.weight\n",
      "prompt_encoder.point_embed.3.weight\n",
      "prompt_encoder.not_a_point_embed.weight\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import monai\n",
    "import torch\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "from statistics import mean\n",
    "from torch.optim import Adam\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import SamModel \n",
    "import matplotlib.patches as patches\n",
    "from transformers import SamProcessor\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from monai.transforms import ResizeD\n",
    "    \n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    LoadImaged,\n",
    "    CenterSpatialCropd,\n",
    "    Invertd,\n",
    "    OneOf,\n",
    "    Orientationd,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    CenterSpatialCropd,\n",
    "    RandSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RepeatChanneld,\n",
    "    ToTensord,\n",
    ")\n",
    "           \n",
    "# create an instance of the processor for image preprocessing\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    '''\n",
    "    This function creates varying bounding box coordinates based on the segmentation contours as prompt for the SAM model\n",
    "    The padding is random int values between 5 and 20 pixels\n",
    "    '''\n",
    "\n",
    "    if len(np.unique(ground_truth_map)) > 1:\n",
    "\n",
    "        # get bounding box from mask\n",
    "        y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        \n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = ground_truth_map.shape\n",
    "        x_min = max(0, x_min - np.random.randint(5, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(5, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(5, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(5, 20))\n",
    "        \n",
    "        bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        return bbox\n",
    "    else:\n",
    "        return [0, 0, 512, 512] # if there is no mask in the array, set bbox to image size\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, processor):\n",
    "        \n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.processor = processor\n",
    "        self.transforms = transforms = Compose([\n",
    "            \n",
    "            # load .nii or .nii.gz files\n",
    "            LoadImaged(keys=['img', 'label']),\n",
    "            \n",
    "            # add channel id to match PyTorch configurations\n",
    "            EnsureChannelFirstd(keys=['img', 'label']),\n",
    "            \n",
    "            # reorient images for consistency and visualization\n",
    "            Orientationd(keys=['img', 'label'], axcodes='RA'),\n",
    "            \n",
    "            # resample all training images to a fixed spacing\n",
    "#             Spacingd(keys=['img', 'label'], pixdim=(1.5, 1.5), mode=(\"bilinear\", \"nearest\")),\n",
    "            \n",
    "            # rescale image and label dimensions to 256x256 \n",
    "            # CenterSpatialCropd(keys=['img', 'label'], roi_size=(256,256)),\n",
    "            ResizeD(keys=['img', 'label'], spatial_size=(256,256), mode=(\"bilinear\", \"nearest\")),\n",
    "            ScaleIntensityd(keys=['img']),\n",
    "            ScaleIntensityRanged(keys=['img'], a_min=0.0, a_max=1.0, \n",
    "                         b_min=0.0, b_max=255.0, clip=True), \n",
    "            ScaleIntensityd(keys=['label']),\n",
    "            # # scale intensities to 0 and 255 to match the expected input intensity range\n",
    "            # ScaleIntensityRanged(keys=['img'], a_min=-1000, a_max=2000, \n",
    "            #              b_min=0.0, b_max=255.0, clip=True), \n",
    "            \n",
    "            # ScaleIntensityRanged(keys=['label'], a_min=0, a_max=1, \n",
    "            #              b_min=0.0, b_max=1.0, clip=True), \n",
    "\n",
    "#             SpatialPadd(keys=[\"img\", \"label\"], spatial_size=(256,256))\n",
    "#             RepeatChanneld(keys=['img'], repeats=3, allow_missing_keys=True)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # create a dict of images and labels to apply Monai's dictionary transforms\n",
    "        data_dict = self.transforms({'img': image_path, 'label': mask_path})\n",
    "\n",
    "        # squeeze extra dimensions\n",
    "        image = data_dict['img'].squeeze()\n",
    "        ground_truth_mask = data_dict['label'].squeeze()\n",
    "\n",
    "        # convert to int type for huggingface's models expected inputs\n",
    "        image = image.astype(np.uint8)\n",
    "        rotated = np.rot90(image, -1)\n",
    "\n",
    "            # 左右翻转\n",
    "        flipped = np.fliplr(rotated)\n",
    "\n",
    "        # convert the grayscale array to RGB (3 channels)\n",
    "        array_rgb = np.dstack((flipped, flipped, flipped))\n",
    "        \n",
    "        # convert to PIL image to match the expected input of processor\n",
    "        image_rgb = Image.fromarray(array_rgb)\n",
    "        \n",
    "        # get bounding box prompt (returns xmin, ymin, xmax, ymax)\n",
    "        # in this dataset, the contours are -1 so we change them to 1 for label and 0 for background\n",
    "        ground_truth_mask[ground_truth_mask < 0] = 1\n",
    "        \n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        \n",
    "        # prepare image and prompt for the model\n",
    "        inputs = self.processor(image_rgb, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # add ground truth segmentation (ground truth image size is 256x256)\n",
    "        inputs[\"ground_truth_mask\"] = torch.from_numpy(ground_truth_mask.astype(np.int8))\n",
    "        inputs[\"image_name\"] = os.path.basename(image_path).replace('.nii.gz', '.png') # Change the extension\n",
    "\n",
    "        return inputs\n",
    "# Initialize dictionary for storing image and label paths\n",
    "data_paths = {}\n",
    "datasets = ['train', 'val', 'test']\n",
    "data_types = ['2d_images', '2d_masks']\n",
    "# Create directories and print the number of images and masks in each\n",
    "for dataset in datasets:\n",
    "    for data_type in data_types:\n",
    "        # Construct the directory path\n",
    "        dir_path = os.path.join('./LUNA16/dataset_512/', f'{dataset}_{data_type}_512')\n",
    "        \n",
    "        # Find images and labels in the directory\n",
    "        files = sorted(glob.glob(os.path.join(dir_path, \"*.nii.gz\")))\n",
    "        \n",
    "        # Store the image and label paths in the dictionary\n",
    "        data_paths[f'{dataset}_{data_type.split(\"_\")[1]}'] = files\n",
    "\n",
    "print('Number of training images', len(data_paths['train_images']))\n",
    "print('Number of validation images', len(data_paths['val_images']))\n",
    "print('Number of test images', len(data_paths['test_images']))\n",
    "# create train and validation dataloaders\n",
    "# train_dataset = SAMDataset(image_paths=data_paths['train_images'], mask_paths=data_paths['train_masks'], processor=processor)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# val_dataset = SAMDataset(image_paths=data_paths['val_images'], mask_paths=data_paths['val_masks'], processor=processor)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "# example = train_dataset[5]\n",
    "# for k,v in example.items():\n",
    "#     print(k,v.shape)\n",
    "\n",
    "# xmin, ymin, xmax, ymax = get_bounding_box(example['ground_truth_mask'])\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# axs[0].imshow(example['pixel_values'][1], cmap='gray')\n",
    "# axs[0].axis('off')\n",
    "\n",
    "# axs[1].imshow(example['ground_truth_mask'], cmap='copper')\n",
    "\n",
    "# # create a Rectangle patch for the bounding box\n",
    "# rect = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "# # add the patch to the second Axes\n",
    "# axs[1].add_patch(rect)\n",
    "\n",
    "# axs[1].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# load the pretrained weights for finetuning\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-large\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder (encoder weights are frozen)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        print(name)\n",
    "        param.requires_grad_(False)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm()\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create test dataloader\n",
    "test_dataset = SAMDataset(image_paths=data_paths['test_images'], mask_paths=data_paths['test_masks'], processor=processor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, binary_dilation, sum as ndi_sum\n",
    "import cv2\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "state_dict = torch.load(\"best_weights_l_v2.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/134456 [00:03<16:18:58,  2.29it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./SAM_label/'):\n",
    "    os.makedirs('./SAM_label/')\n",
    "with torch.no_grad():\n",
    "    cnt=0\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        cnt+=1\n",
    "        if cnt<10:\n",
    "            # continue\n",
    "            # forward pass\n",
    "            # 向右旋转90°\n",
    "\n",
    "\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"].cuda(0),\n",
    "                            input_boxes=batch[\"input_boxes\"].cuda(0),\n",
    "                            multimask_output=False)\n",
    "            # compute loss\n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().cuda(1)\n",
    "            # apply sigmoid\n",
    "            medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "            # convert soft mask to hard mask\n",
    "            medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "            medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "            # print(medsam_seg.shape)\n",
    "            # print(medsam_seg.dtype)            \n",
    "            # Remove regions connected to the image border\n",
    "            labeled, num_features = label(medsam_seg)\n",
    "            border_mask = np.zeros_like(medsam_seg)\n",
    "            border_mask[:, 0] = 1\n",
    "            border_mask[0, :] = 1\n",
    "            border_mask[:, -1] = 1\n",
    "            border_mask[-1, :] = 1\n",
    "            touching_border = np.unique(labeled * binary_dilation(border_mask))\n",
    "            for region in touching_border:\n",
    "                if region != 0:\n",
    "                    medsam_seg[labeled == region] = 0                    \n",
    "            # plt.imshow(batch[\"pixel_values\"][0, 1], cmap='gray')\n",
    "            # plt.show()\n",
    "            # Visualize the image after preprocessing (before Canny)\n",
    "            # plt.imshow(medsam_seg, cmap='copper')\n",
    "            # plt.title('before Canny')\n",
    "            # plt.show()\n",
    "\n",
    "            \n",
    "            # 使用Canny边缘检测\n",
    "            original_image = batch[\"pixel_values\"][0, 1].cpu().numpy().astype(np.uint8)\n",
    "            edges_original = cv2.Canny(original_image, 50, 200)\n",
    "            # plt.imshow(edges_original, cmap='gray')\n",
    "            # plt.title('Canny Edges of Original Image')\n",
    "            # plt.show()\n",
    "            kernel = np.ones((3,3),np.uint8)\n",
    "            dilated = cv2.dilate(edges_original, kernel, iterations=1)\n",
    "            # plt.imshow(dilated,cmap='gray')\n",
    "            # plt.show()\n",
    "            # 找到所有的轮廓\n",
    "            contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            # print(contours)\n",
    "            # 找到最大的轮廓\n",
    "            max_contour = max(contours, key=cv2.contourArea)\n",
    "            # filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 10]\n",
    "            # 创建一个全黑的图像\n",
    "            mask = np.zeros_like(edges_original)\n",
    "            # 在mask上画出最大的轮廓\n",
    "            cv2.drawContours(mask, [max_contour],-1, (255), thickness=cv2.FILLED)\n",
    "            # 创建一个比输入图像大2的掩码，用于floodFill函数\n",
    "            h, w = mask.shape[:2]\n",
    "            mask_floodfill = np.zeros((h+2, w+2), np.uint8)\n",
    "            # floodFill函数会改变输入图像，所以我们使用它的副本\n",
    "            mask_floodfill_copy = mask.copy()\n",
    "            # 找到一个种子点\n",
    "            seed_point = (w//2, h//2)\n",
    "            # 执行floodFill函数，将与种子点连通的区域填充为白色\n",
    "            cv2.floodFill(mask_floodfill_copy, mask_floodfill, seed_point, 255)\n",
    "            # 结合原始的mask和floodfill的结果，得到最终的mask\n",
    "            final_mask = mask | mask_floodfill_copy\n",
    "            final_mask = cv2.resize(final_mask, (256, 256))\n",
    "            # Keep regions with area greater than 2*2\n",
    "            for i in range(1, num_features + 1):\n",
    "                area = ndi_sum(labeled == i)\n",
    "                if area <= 4:\n",
    "                    medsam_seg[labeled == i] = 0\n",
    "            # plt.imshow(final_mask,cmap='gray')\n",
    "            # plt.title('mask')\n",
    "            # plt.show()\n",
    "            # 使用mask去掉落在外边的像素点\n",
    "            masked_seg = cv2.bitwise_or(medsam_seg,medsam_seg, mask=final_mask)\n",
    "            closed_seg = cv2.morphologyEx(masked_seg, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "            # plt.figure(figsize=(12,4))\n",
    "            # plt.subplot(1,3,1)\n",
    "            # plt.imshow(batch[\"pixel_values\"][0,1], cmap='gray')\n",
    "            # plt.title('original_image')\n",
    "            # plt.axis('off')\n",
    "            # plt.subplot(1,3,2)\n",
    "            # plt.imshow(batch[\"ground_truth_mask\"][0], cmap='copper')\n",
    "            # plt.title('ground_truth_masks')\n",
    "            # plt.axis('off')\n",
    "            # plt.subplot(1,3,3)\n",
    "            # plt.imshow(closed_seg, cmap='copper')\n",
    "            # plt.title('after_canny')\n",
    "            # plt.axis('off')\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "            # 保存图像\n",
    "            result_image_name = os.path.join('./SAM_label/', batch[\"image_name\"][0])\n",
    "            result_image = Image.fromarray((closed_seg).astype(np.uint8))\n",
    "            result_image.save(result_image_name)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        # if cnt>55:\n",
    "        #     break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
